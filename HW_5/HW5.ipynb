{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW5.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "rQGIRVp8BSPW",
        "j2Ygl9gTBYob",
        "2v9LjXkwTTn7",
        "fSGkruqcrHbN"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arrowstoflight/AI/blob/master/HW_5/HW5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tKRYhFRUAlbU"
      },
      "source": [
        "Dustin White\n",
        "\n",
        "CAP4630 - Artificial Intelligence\n",
        "\n",
        "Professor Wocjan\n",
        "\n",
        "15 April 2020\n",
        "\n",
        "Homework 5\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQGIRVp8BSPW",
        "colab_type": "text"
      },
      "source": [
        "# 1. General Concepts\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXMfzUO97HO8",
        "colab_type": "text"
      },
      "source": [
        "**Artificial Intelligence:** A computer system able to perform tasks that normally require human intelligence, such as visual perception, speech recognition, decision-making, and translation between languages [1].\n",
        "\n",
        "**Machine Learning:** The field of study that gives computers the ability to learn without being explicitly programmed [1]. \n",
        "\n",
        "**Deep Learning:** A subset of machine learning , and machine learning is a subset of AI, which is an umbrella term for any computer program that does something smart [1].\n",
        "\n",
        "A **nueral network** consists of an input layer and an output layer, with one or many hidden layers in between.\n",
        "\n",
        "![alt text](https://miro.medium.com/max/1400/1*eJ36Jpf-DE9q5nKk67xT0Q.jpeg)\n",
        "\n",
        "Figure 1: Neural Network [2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "j2Ygl9gTBYob"
      },
      "source": [
        "# 2. Basic Concepts\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9a-BNuPQAit7",
        "colab_type": "text"
      },
      "source": [
        "A **Linear Regression model** can be used predict to a dependent variable given an independent variable.\n",
        "\n",
        "$\\hat{y}$ = b + w<sub>1</sub>x<sub>1</sub>, \n",
        "where:\n",
        "*   $\\hat{y}$ is the predicted label (a desired output)\n",
        "*   b is the bias (the y-intercept), sometimes referred to as w0\n",
        "*   w<sub>1</sub> is the weight of feature 1. Weight is the same concept as the \"slope\" m in the traditional equation of a line\n",
        "*   x<sub>1</sub> is a feature (a known input)\n",
        "\n",
        "**Logistic regression** is a classification algorithm used to assign observations to a discrete set of classes. It is used for classification problems, it is a predictive analysis algorithm and based on the concept of probablity [6]. \n",
        "\n",
        "**Gradients** captures all the partial derivatives of a multi-variable function [4].\n",
        "\n",
        "**Gradient descent** is an optimization algorithm used to minimize some function by iteratively moving in the direction of steepest descent as defined by the negative of the gradient [5].\n",
        "\n",
        "![alt text](https://ml-cheatsheet.readthedocs.io/en/latest/_images/gradient_descent_demystified.png)\n",
        "\n",
        "**Learning Rate** is used by the gradient descent algorithm by multiplying the gradient by the learning rate. If the learning rate is too small, learning will take too long. If the learning rate is too high, it could overshoot a minimum value [1]."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LlZBkiUcBbtc"
      },
      "source": [
        "# 3. Building a model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnFrO8C6zqUL",
        "colab_type": "text"
      },
      "source": [
        "**Convolutional Neural Networks or (CNN)** are used in machine learning for image classiification. It consists of an input layer and output layer, with one or more hidden layers in between.\n",
        "\n",
        "The convolutional bases consists of convolution modules (convolutional layers + pooling layers) followed by a classifier\n",
        "\n",
        "Firstly, to **create the model** we use the following code snipet:\n",
        "\n",
        "> model = models.Sequential()\n",
        "\n",
        "Next, we will add our model **layers**. The following code snipet will add an input layer with an input_shape of 3,3,1 which represents 3x3 image:\n",
        "\n",
        ">model.add(Dense(10, activation='relu', input_shape=(3,3,1)))  \n",
        ">model.add(Dense(10, activation='relu'))\n",
        "\n",
        "Finally the following code snippit adds the output layer and displays the summary of the model\n",
        "\n",
        ">model.add(Dense(1))  \n",
        ">model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AwxuSspPy84z"
      },
      "source": [
        "# 4. Compiling a model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNtr-mS5zxHX",
        "colab_type": "text"
      },
      "source": [
        "In order to compile a model, the model takes in two parameters:\n",
        "\n",
        "The **optimizer** determines how the network will be updated based on the loss function. It implements a specific variant of stochastic gradient descent (SGD)[1].\n",
        "\n",
        "and\n",
        "\n",
        "**Learning Rate:** which determines how fast the optimal weights for the model are calculated [2].\n",
        "\n",
        "The final part of the compilation is the **loss function**. This is calculated using 'mean squared error'. Which takes the average squared difference between the predicited and actual values, and is ideal for regression problems. A good model will have a loss close to zero [2].\n",
        "\n",
        "![alt text](https://miro.medium.com/max/620/1*eld-eBTZ-LMLTQgaAjrU8w.png)\n",
        "\n",
        "Figure 4.1: Loss function [2]\n",
        "\n",
        "The following code snipit will compile the model:\n",
        "\n",
        "> model.compile(  \n",
        "    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;loss='binary_crossentropy',  \n",
        "    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;optimizer=optimizers.RMSprop(lr=2e-5),  \n",
        "    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;metrics=['acc'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BcxMp9CLy8Sh"
      },
      "source": [
        "# 5. Training a Model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNFxfA5u9UJb",
        "colab_type": "text"
      },
      "source": [
        "**Overfitting** refers to a model that models the training data too well. Is caused by a model learning the detail and noise of the training data set \"too well\". The noise or random fluctuations of training data is picked up by the model and negativly impacts the results [3].\n",
        "\n",
        "**Underfitting** refers to a model that can neither model the training data nor interpret the data correctly[3]. Underfitting can lead to unreliable predictions.\n",
        "\n",
        "**Epochs** refers to the number of passes through the entire training dataset that the learning algorithm has completed [7].\n",
        "\n",
        "\n",
        "The following code snippit is used to train the model:\n",
        "\n",
        ">history = model.fit_generator(  \n",
        ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; train_generator,  \n",
        ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; steps_per_epoch=100,  \n",
        ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; epochs=2,  \n",
        ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; validation_data=validation_generator,  \n",
        ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; validation_steps=50  \n",
        ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; verbose=2)  \n",
        "\n",
        "Figure 5.1 Code referenced from [1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "r25xgTfLz6hX"
      },
      "source": [
        "# 6. Finetuning a pretrained model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRiJIoZE0AGF",
        "colab_type": "text"
      },
      "source": [
        "Finetuning is a way to further increase perfomance of a model by \"fine-tuning\" the weights of the top layers of the pretrained model alongside the training of the top-level classifier. \n",
        "\n",
        "*   **It should only be attempted after you have trained the top-level classifer with the pretrained model set to non-trainable.**\n",
        "*   Additionally, we **fine-tune only the top layers of the pre-trained model** rather than all layers of the pretrained model because, in a convnet, the higher up a layer is, the more specialized it is [1].\n",
        "\n",
        "In order to implement fine-tuning one must set the top layers of InceptionV3 to be trainable, recompile the model, and resume training. The following code snippit can help see how this works:\n",
        "\n",
        ">unfreeze = False  \n",
        ">\n",
        ">for layer in pre_trained_model.layers:  \n",
        ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if unfreeze:  \n",
        ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;    layer.trainable = True  \n",
        ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if layer.name == 'mixed6':  \n",
        ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;    unfreeze = True  \n",
        ">\n",
        ">model.compile(loss='binary_crossentropy',  \n",
        ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;              optimizer=SGD(  \n",
        ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;                  lr=0.00001,   \n",
        ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;                  momentum=0.9),  \n",
        ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;              metrics=['acc'])\n",
        ">\n",
        ">history = model.fit_generator(  \n",
        ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; train_generator,    \n",
        ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; steps_per_epoch=100,  \n",
        ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; epochs=50,  \n",
        ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; validation_data=validation_generator,  \n",
        ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  validation_steps=50,  \n",
        ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  verbose=2)\n",
        "\n",
        "Figure 6.1 Code referenced from [1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KmZ0AY5zp2tM"
      },
      "source": [
        "# 7. References\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEuTpKY1p8_u",
        "colab_type": "text"
      },
      "source": [
        "**[1] Professor Wocjan's Github Notes:** https://github.com/schneider128k/machine_learning_course\n",
        "\n",
        "**[2] Building A Deep Learning Model using Keras:** https://towardsdatascience.com/building-a-deep-learning-model-using-keras-1548ca149d37\n",
        "\n",
        "**[3] Overfitting and Underfitting With Machine Learning Algorithms:** [https://machinelearningmastery.com/overfitting-and-underfitting-with-machine-learning-algorithms/](https://machinelearningmastery.com/overfitting-and-underfitting-with-machine-learning-algorithms/)\n",
        "\n",
        "**[4] Machine Learning 101: An Intuitive Introduction to Gradient Descent:** https://towardsdatascience.com/machine-learning-101-an-intuitive-introduction-to-gradient-descent-366b77b52645\n",
        "\n",
        "**[5] Gradient Descent:** https://ml-cheatsheet.readthedocs.io/en/latest/gradient_descent.html\n",
        "\n",
        "**[6] Introduction to Logistic Regression:** https://towardsdatascience.com/introduction-to-logistic-regression-66248243c148\n",
        "\n",
        "**[7] Epoch (Machine Learning):** https://radiopaedia.org/articles/epoch-machine-learning"
      ]
    }
  ]
}